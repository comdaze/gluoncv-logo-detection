{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GluonCV on SageMaker demo\n",
    "\n",
    "Let's first import the necessary modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gluoncv --pre -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2\n",
    "import boto3\n",
    "import json\n",
    "import mxnet as mx\n",
    "from gluoncv import model_zoo, data, utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection with a pre-trained SSD model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo we are going to perform inference on a GluonCV object detection model that has been deployed in Amazon SageMaker. We run this notebook on our laptop to capture image data from the webcam and to send it to the SageMaker endpoint. \n",
    "\n",
    "The endpoint is running the model `ssd_512_resnet50_v1_voc` which has the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSD(\n",
       "  (features): FeatureExpander(\n",
       "  \n",
       "  )\n",
       "  (class_predictors): HybridSequential(\n",
       "    (0): ConvPredictor(\n",
       "      (predictor): Conv2D(None -> 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1): ConvPredictor(\n",
       "      (predictor): Conv2D(None -> 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (2): ConvPredictor(\n",
       "      (predictor): Conv2D(None -> 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (3): ConvPredictor(\n",
       "      (predictor): Conv2D(None -> 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (4): ConvPredictor(\n",
       "      (predictor): Conv2D(None -> 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (5): ConvPredictor(\n",
       "      (predictor): Conv2D(None -> 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (box_predictors): HybridSequential(\n",
       "    (0): ConvPredictor(\n",
       "      (predictor): Conv2D(None -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1): ConvPredictor(\n",
       "      (predictor): Conv2D(None -> 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (2): ConvPredictor(\n",
       "      (predictor): Conv2D(None -> 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (3): ConvPredictor(\n",
       "      (predictor): Conv2D(None -> 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (4): ConvPredictor(\n",
       "      (predictor): Conv2D(None -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (5): ConvPredictor(\n",
       "      (predictor): Conv2D(None -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (anchor_generators): HybridSequential(\n",
       "    (0): SSDAnchorGenerator(\n",
       "    \n",
       "    )\n",
       "    (1): SSDAnchorGenerator(\n",
       "    \n",
       "    )\n",
       "    (2): SSDAnchorGenerator(\n",
       "    \n",
       "    )\n",
       "    (3): SSDAnchorGenerator(\n",
       "    \n",
       "    )\n",
       "    (4): SSDAnchorGenerator(\n",
       "    \n",
       "    )\n",
       "    (5): SSDAnchorGenerator(\n",
       "    \n",
       "    )\n",
       "  )\n",
       "  (bbox_decoder): NormalizedBoxCenterDecoder(\n",
       "  \n",
       "  )\n",
       "  (cls_decoder): MultiPerClassDecoder(\n",
       "  \n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = model_zoo.get_model('ssd_512_resnet50_v1_voc', pretrained=False)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step we assume that the AWS credentials have been set beforehand. If this is not the case, make sure to set the following environment variables in the commandline before starting this notebook: `AWS_ACCESS_KEY_ID` `AWS_SECRET_ACCESS_KEY` and `AWS_SESSION_TOKEN`.\n",
    "\n",
    "In the following lines of code, we list the endpoints that have been deployed in our account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'EndpointName': 'gluoncv-bcbs-logo-detection',\n",
       "  'EndpointArn': 'arn:aws:sagemaker:us-east-1:291790195320:endpoint/gluoncv-bcbs-logo-detection',\n",
       "  'CreationTime': datetime.datetime(2019, 12, 6, 19, 0, 55, 278000, tzinfo=tzlocal()),\n",
       "  'LastModifiedTime': datetime.datetime(2019, 12, 6, 19, 8, 14, 469000, tzinfo=tzlocal()),\n",
       "  'EndpointStatus': 'InService'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtime= boto3.client('runtime.sagemaker')\n",
    "client = boto3.client('sagemaker')\n",
    "client.list_endpoints()['Endpoints']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained SSD model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get the endpoint that has been deployed under the name `gluoncv-pretrained-object-detection-ssd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for endpoint in  client.list_endpoints()['Endpoints']:\n",
    "    if endpoint[\"EndpointName\"] == \"gluoncv-bcbs-logo-detection\":\n",
    "        gluoncv_endpoint = endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we capture image data from the local webcam and send it to the endpoint. The endpoint will perform inference and return bounding boxes, class ids and confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    img = cv2.resize(frame, (512,512))\n",
    "    payload = json.dumps(img.tolist())\n",
    "    response = runtime.invoke_endpoint(EndpointName=gluoncv_endpoint['EndpointName'],\n",
    "                                       Body=payload)\n",
    "    response_body = response['Body']\n",
    "    result = json.loads(response_body.read().decode())\n",
    "    [class_IDs, scores, bounding_boxes] = result\n",
    "    \n",
    "    bounding_boxes, scores, class_IDs =  mx.nd.array(bounding_boxes), mx.nd.array(scores), mx.nd.array(class_IDs)\n",
    "    ax = utils.viz.cv_plot_bbox(img, bounding_boxes[0], scores[0], class_IDs[0], thresh=0.6, class_names=net.classes)\n",
    "    \n",
    "    cv2.imshow(\"image\", img)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Logo detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have deployed the GluonCV object detection model before in SageMaker under the name `gluoncv-demo-object-detection`. Now we retrieve this endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'gluoncv-bcbs-logo-detection',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-east-1:291790195320:endpoint/gluoncv-bcbs-logo-detection',\n",
       " 'CreationTime': datetime.datetime(2019, 12, 7, 4, 15, 21, 418000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2019, 12, 7, 4, 23, 36, 915000, tzinfo=tzlocal()),\n",
       " 'EndpointStatus': 'InService'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtime= boto3.client('runtime.sagemaker')\n",
    "client = boto3.client('sagemaker')\n",
    "client.list_endpoints()['Endpoints']\n",
    "\n",
    "for endpoint in  client.list_endpoints()['Endpoints']:\n",
    "    if endpoint[\"EndpointName\"] == \"gluoncv-bcbs-logo-detection\":\n",
    "        gluoncv_endpoint = endpoint\n",
    "        \n",
    "gluoncv_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code section, we retrieve frames from the webcam and send it to the SageMaker endpoint. We first have to convert the data into JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import json\n",
    "cap = cv2.VideoCapture(0)\n",
    "time.sleep(2)\n",
    "\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    img = cv2.resize(frame, (512,512))\n",
    "    payload = json.dumps(img.tolist())\n",
    "    response = runtime.invoke_endpoint(EndpointName=gluoncv_endpoint['EndpointName'],\n",
    "                                       Body=payload)\n",
    "    response_body = response['Body']\n",
    "    result = json.loads(response_body.read().decode())\n",
    "    [class_IDs, scores, bounding_boxes] = result\n",
    "    \n",
    "    bounding_boxes, scores, class_IDs =  mx.nd.array(bounding_boxes), mx.nd.array(scores), mx.nd.array(class_IDs)\n",
    "    ax = utils.viz.cv_plot_bbox(img, bounding_boxes[0], scores[0], class_IDs[0], thresh=0.6, class_names=[\"logo\"])\n",
    "    \n",
    "    cv2.imshow(\"image\", img)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "/feedstock_root/build_artefacts/opencv_1520639543262/work/opencv-3.3.0/modules/imgproc/src/color.cpp:10638: error: (-215) scn == 3 || scn == 4 in function cvtColor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-db48bc6d3aa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Our operations on the frame come here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Display the resulting frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /feedstock_root/build_artefacts/opencv_1520639543262/work/opencv-3.3.0/modules/imgproc/src/color.cpp:10638: error: (-215) scn == 3 || scn == 4 in function cvtColor\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import glob\n",
    "\n",
    "train_images = glob.glob(\"images/*.jpg\")\n",
    "\n",
    "n_images = 12\n",
    "cols = (int(math.sqrt(n_images)))*2\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "imgs = []\n",
    "for n, (image) in enumerate(train_images[:n_images]):\n",
    "    img = cv2.imread(image)\n",
    "    img = cv2.resize(img, (512,512))\n",
    "    payload = json.dumps(img.tolist())\n",
    "    response = runtime.invoke_endpoint(EndpointName=gluoncv_endpoint['EndpointName'],\n",
    "                                       Body=payload)\n",
    "    response_body = response['Body']\n",
    "    result = json.loads(response_body.read().decode())\n",
    "    [class_IDs, scores, bounding_boxes] = result\n",
    "   \n",
    "    bounding_boxes, scores, class_IDs =  mx.nd.array(bounding_boxes), mx.nd.array(scores), mx.nd.array(class_IDs)\n",
    "    ax = utils.viz.cv_plot_bbox(img, bounding_boxes[0], scores[0], class_IDs[0], thresh=0.6, class_names=[\"logo\"])\n",
    "    img = cv2.resize(img, (700, 700))\n",
    "    #plt.imshow(img)\n",
    "    imgs.append(img)   \n",
    "    \n",
    "w=10\n",
    "h=10\n",
    "fig=plt.figure(figsize=(60, 60))\n",
    "columns = 1\n",
    "rows = 11\n",
    "for i in range(1, columns*rows +1):\n",
    "    #img = np.random.randint(10, size=(h,w))\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(imgs[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <h3>Live Feed</h3>\n",
       "    <video autoplay height=384></video>\n",
       "    <h3>Predicted Feed</h3>\n",
       "    <canvas id=\"annotated\"  width=512 height=384></canvas>\n",
       "    <script>\n",
       "\n",
       "    var video = document.querySelector('video')\n",
       "    var canvas = document.getElementById('annotated')\n",
       "\n",
       "\n",
       "    navigator.mediaDevices.getUserMedia({ video: true })\n",
       "      .then(stream=> video.srcObject = stream)\n",
       "\n",
       "\n",
       "    function handle_output(output) {\n",
       "        console.log(output)\n",
       "        var canvas = document.getElementById('annotated')\n",
       "        var ctx = canvas.getContext(\"2d\");\n",
       "        var image = new Image();\n",
       "        image.onload = function() {\n",
       "           ctx.drawImage(image, 0, 0);\n",
       "        };\n",
       "        image.src = output.content.text\n",
       "        update_image()\n",
       "    }\n",
       "\n",
       "    function update_image() {\n",
       "        var video = document.querySelector('video')\n",
       "        var canvas = document.createElement('canvas')\n",
       "        var [w,h] = [video.offsetWidth, video.offsetHeight]\n",
       "        canvas.width = w\n",
       "        canvas.height = h\n",
       "        canvas.getContext('2d')\n",
       "              .drawImage(video, 0, 0, w, h)\n",
       "        command = 'get_annotated_image(\"' + canvas.toDataURL('image/jpeg', 0.800000) + '\")'\n",
       "\n",
       "        var callbacks = {\n",
       "            iopub : {\n",
       "                 output : handle_output,\n",
       "            }\n",
       "        }\n",
       "        IPython.notebook.kernel.execute(command, callbacks)\n",
       "    }\n",
       "\n",
       "    setTimeout(update_image, 1000);\n",
       "\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from utils import show_webcam\n",
    "\n",
    "def get_annotated_image(input_image_b64):\n",
    "    prefix, input_image_b64 = input_image_b64.split(',')\n",
    "    input_image_binary = BytesIO(base64.b64decode(input_image_b64))\n",
    "    input_image_np = np.asarray(Image.open(input_image_binary))\n",
    "    input_image_np, _ = mx.image.center_crop(mx.nd.array(input_image_np), (512,512))\n",
    "    _, input_image_loaded = gcv.data.transforms.presets.yolo.transform_test(input_image_np, short=384)\n",
    "    \n",
    "    payload = json.dumps(input_image_loaded.tolist())\n",
    "    response = runtime.invoke_endpoint(EndpointName=gluoncv_endpoint['EndpointName'],\n",
    "                                       Body=payload)\n",
    "    response_body = response['Body']\n",
    "    result = json.loads(response_body.read().decode())\n",
    "    [class_IDs, scores, bounding_boxes] = result\n",
    "    bounding_boxes, scores, class_IDs =  mx.nd.array(bounding_boxes), mx.nd.array(scores), mx.nd.array(class_IDs)\n",
    "    output_image_np = gcv.utils.viz.cv_plot_bbox(input_image_loaded, bounding_boxes[0], scores[0], class_IDs[0], thresh=0.6, class_names=[\"logo\"])\n",
    "    \n",
    "    #output = deployed_model.predict(input_image_loaded)\n",
    "    #cid = np.array(output['cid'])\n",
    "    #scores = np.array(output['score'])\n",
    "    #bbox = np.array(output['bbox'])\n",
    "    #output_image_np = gcv.utils.viz.cv_plot_bbox(input_image_loaded, bbox, scores, cid, class_names=classes)\n",
    "    output_image_PIL = Image.fromarray(output_image_np)\n",
    "    output_buffer = BytesIO()\n",
    "    output_image_PIL.save(output_buffer, format=\"JPEG\")\n",
    "    output_image_b64 = 'data:image/jpeg;base64,'+base64.b64encode(output_buffer.getvalue()).decode(\"utf-8\")\n",
    "    print(output_image_b64)\n",
    "\n",
    "show_webcam()\n",
    "#display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.resize(frame, (512,512))\n",
    "    payload = json.dumps(img.tolist())\n",
    "    response = runtime.invoke_endpoint(EndpointName=gluoncv_endpoint['EndpointName'],\n",
    "                                       Body=payload)\n",
    "    response_body = response['Body']\n",
    "    result = json.loads(response_body.read().decode())\n",
    "    [class_IDs, scores, bounding_boxes] = result\n",
    "    \n",
    "    bounding_boxes, scores, class_IDs =  mx.nd.array(bounding_boxes), mx.nd.array(scores), mx.nd.array(class_IDs)\n",
    "    ax = utils.viz.cv_plot_bbox(img, bounding_boxes[0], scores[0], class_IDs[0], thresh=0.6, class_names=[\"logo\"])\n",
    "    \n",
    "    cv2.imshow(\"image\", img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
